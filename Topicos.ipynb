{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de tópicos\n",
    "\n",
    "En esta notebook vamos a descomponer en tópicos un conjunto de textos. La descomposición en tópicos pueden pensarla a la vez como un proceso de reducción dimensional, al describir los datos en el espacio de tópicos en vez del espacio original de features), o como un proceso de *clustering*, al agrupar los textos en dichos tópicos.\n",
    "\n",
    "Vamos a aplicar dos algoritmos (NMF y LDA) sobre el dataset de letras de tango visto en clases anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerías a utilizar \n",
    "import requests # Realizar peticiones al servidor\n",
    "from bs4 import BeautifulSoup as BS #Extrae, proporciona métodos y herramientas para procesar páginas web de manera más sencilla\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "pd.set_option('max_colwidth',150) # para que las columnas del dataframe muestren hasta 150 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.literatura.us/garciamarquez/'\n",
    " # Hacemos la petición HTTP\n",
    "response=requests.get(url)\n",
    "    \n",
    "# soup es un objeto manipulable creado a partir del contenido de la página y BeautifulSoup\n",
    "soup = BS(response.content)\n",
    "\n",
    "# Extraemos el elemento p de la  clase = \"MsoNormal\" dónde se encuentra el cuento \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Encontrar todos los elementos <p> con la clase 'MsoNormal'\n",
    "paragraphs = soup.find_all(class_='MsoNormal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos = []\n",
    "enlaces = []\n",
    "anios   = ['1947', '1949', '1949', '1950', '1950', '1955', '1954', '1961', '1962','1962','1962','1962',\n",
    "           '1962','1962','1962', '1962',\n",
    "          '1968', '1968', '1970', '1972', '1975', '1981', '1979', '1981', \n",
    "           '1982', '1980', '1978', '1980', '1979', '1980', '1982', '1978', '1978', '1976','1967','1967'\n",
    "           ,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]\n",
    "\n",
    "for p in paragraphs:\n",
    "    # Buscar solo los elementos <a> directamente dentro del elemento <p>\n",
    "    a_tags = p.find_all('a')\n",
    "    texto_p =p.text.strip()\n",
    "    for a in a_tags:\n",
    "        titulos.append(a.text.strip())\n",
    "        enlaces.append(a['href'])\n",
    "    \n",
    "\n",
    "cuentos={\n",
    "         'titulos': titulos,\n",
    "         'enlaces': enlaces,\n",
    "         'años' : anios\n",
    "}\n",
    "\n",
    "cuentos=pd.DataFrame(cuentos)\n",
    "\n",
    "cuentos.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "for url in cuentos.enlaces:\n",
    "    # Hacemos la petición HTTP\n",
    "    response=requests.get(url)\n",
    "    \n",
    "    # soup es un objeto manipulable creado a partir del contenido de la página y BeautifulSoup\n",
    "    soup = BS(response.content)\n",
    "\n",
    "    # Extraemos el elemento p de la  clase = \"MsoNormal\" dónde se encuentra el cuento \n",
    "    paragraph= soup.find(attrs = {\"class\": \"MsoNormal\"}).text\n",
    "\n",
    "    # Palabras a quitar del párrafo\n",
    "    palabras_remplazar='Literatura\\n.us\\nMapa de la biblioteca | Aviso Legal | Quiénes Somos | Contactar\\n      \\xa0\\n\\n\\n\\n'\n",
    "\n",
    "    # Quitamos palabras de publicidad que están en párrafo \n",
    "    paragraphs = paragraph.replace(palabras_remplazar, \"\")\n",
    "    \n",
    "    # Iteramos para todo párrafo y vemos el texto contenido\n",
    "    # Podemos hacer una lista y concatenar todo con un salto de línea\n",
    "    # paragraphs= '\\n \\n'.join(paragraphs)\n",
    "\n",
    "    # Adicionamos cada texto al diccionario texts, y lo relacionamos con su título \n",
    "    text.append(paragraphs)\n",
    "cuentos['textos']=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re       # librería de expresiones regulares\n",
    "import string   # librería de cadena de caracteres\n",
    "# Defino una función que recibe un texto y devuelve el mismo texto sin singnos,\n",
    "def clean_text_round1(text):\n",
    "    # pasa las mayúsculas del texto a minúsculas\n",
    "    text = text.lower() \n",
    "\n",
    "    # reemplaza texto entre corchetes por espacio en blanco.. ¿ y \\% no se..\n",
    "    text = re.sub(r'\\[.*?¿\\]\\%', ' ', text)\n",
    "\n",
    "    #Eliminar tanto los espacios en blanco no rompibles (\"\\xa0\"), saltos de línea y carácter \"\\xad\"\n",
    "    text=re.sub(r'[\\xa0\\n¿!]', '', text) \n",
    "\n",
    "    #Eliminar el carácter \"\\xad\"\n",
    "    text=re.sub(r'[\\xad]', '', text) \n",
    "\n",
    "    #Eliminar los retornos de carro (\"\\r\") \n",
    "    text=re.sub(r'[\\r]', ' ', text)  \n",
    "\n",
    "    # Eliminar guiones (-), en dash (–) y em dash (—)\n",
    "    text = re.sub(r'[-\\u2013\\u2014]', '', text) \n",
    "\n",
    "    text=re.sub(r'<(?!\\/?[a-zA-Z])', '', text)\n",
    "\n",
    "    # reemplaza singnos de puntuación por espacio en blanco.. %s -> \\S+ es cualquier caracter que no sea un espacio en blanco\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) \n",
    "\n",
    "    # Sacamos comillas, los puntos suspensivos, <<, >>\n",
    "    text = re.sub('[‘’“”…«»]', '', text)\n",
    "\n",
    "    # remueve palabras que contienen números.\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)        \n",
    "\n",
    "    return text\n",
    "\n",
    "# Defino una función anónima que al pasarle un argumento devuelve el resultado de aplicarle la función anterior a este mismo argumento\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "\n",
    "# Dataframe que resulta de aplicarle a las columnas la función de limpieza\n",
    "# Aplicar la limpieza a las columnas deseadas\n",
    "cuentos['textos'] = cuentos['textos'].apply(clean_text_round1)\n",
    "cuentos['titulos'] = cuentos['titulos'].apply(clean_text_round1)\n",
    "\n",
    "#Lo pickleamos y guardamos \n",
    "cuentos.to_pickle(r\"C:\\Users\\HP\\Documents\\Ciencia De Datos\\Análisis de los funerales de la mamá grande\\corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Librería para hacer wordclouds\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwordcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m WordCloud\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importamos nltk para extraer stopwords \n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Librería para hacer wordclouds\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetos de sklearn para hacer tópicos\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Contador de frecuencia\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # Creador de tf-idf\n",
    "\n",
    "# Algoritmos de descomposición de tópicos\n",
    "from sklearn.decomposition import NMF \n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de la matriz documentos-términos\n",
    "\n",
    "Vamos a construir esta matriz con valorización tf-idf, es decir, además de la frecuencia del término vamos a ponderar la especificidad. \n",
    "Esto lo hacemos en dos pasos: primero, describimos nuestros datos mediante frecuencia de términos; luego, le agregamos la valorización de la especificidad. \n",
    "Vamos además a remover las *stopwords* obtenidas de *nltk*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# Creamos el objeto contador de palabras, pidiéndole que remueve\n",
    "# las stopwords, los términos que aparecen en un único documento (min_df)\n",
    "# y los términos que aparecen en más del 70% de los documentos (max_df).\n",
    "# Esto es para eliminar palabras raras (o errores de tipeo) y \n",
    "# términos que seguramente son stopwords no incluídos en la lista\n",
    "count = CountVectorizer(min_df = 2, max_df = 0.70, stop_words = stopwords)\n",
    "\n",
    "# Ajustamos con los datos. Acá especificamente creamos una matriz documentos-términos\n",
    "x_count = count.fit_transform(df['letra'])\n",
    "\n",
    "# Dimensions de la matriz doc-tér\n",
    "print(x_count.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos tranquilamente trabajar sobre la matriz de documentos descritos a través de la frecuencia de las palabras o bien, como es habitual, ponderar la especificidad de los términos mediante tf-idf. Esto lo hacemos de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto tf-idf. Le decimos además que devuelva los\n",
    "# vectores documento con norma euclídea igual a 1 (norm = 'l2')\n",
    "tfidf = TfidfTransformer(norm = 'l2')\n",
    "\n",
    "# Creamos la matriz tf-idf a partir de la matriz de frecuencias\n",
    "x_tfidf = tfidf.fit_transform(x_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación de NMF sobre el corpus\n",
    "\n",
    "Vamos a buscar los tópicos en nuestro corpus de textos a través de la descomposición en matrices no-negativas. Elijamos por ejemplo 5 tópicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elijamos la cantidad de tópicos\n",
    "n_components = 5\n",
    "\n",
    "# Construímos el objeto NMF con los tópicos indicados \n",
    "nmf = NMF(n_components = n_components)\n",
    "\n",
    "# Aplicamos sobre nuestros datos\n",
    "x_nmf = nmf.fit_transform(x_tfidf)\n",
    "\n",
    "# Dimensión de la matriz transformada\n",
    "print(x_nmf.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué significa cada tópico. Lo que hay que indentificar es cuáles son los índices con mayor peso en cada componente y a qué término le corresponde. Para ello primero vamos a invertir el diccionario de vocabulario (para obtener otro del estilo \"índice: término\") y ordenar los índices de mayor a menor en cada componente:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
