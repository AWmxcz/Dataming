{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de tópicos\n",
    "\n",
    "En esta notebook vamos a descomponer en tópicos un conjunto de textos. La descomposición en tópicos pueden pensarla a la vez como un proceso de reducción dimensional, al describir los datos en el espacio de tópicos en vez del espacio original de features), o como un proceso de *clustering*, al agrupar los textos en dichos tópicos.\n",
    "\n",
    "Vamos a aplicar dos algoritmos (NMF y LDA) sobre el dataset de libros de Gabriel García Márquez visto en clases anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías habituales\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos nltk para extraer stopwords \n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Librería para hacer wordclouds\n",
    "#from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetos de sklearn para hacer tópicos\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Contador de frecuencia\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # Creador de tf-idf\n",
    "\n",
    "# Algoritmos de descomposición de tópicos\n",
    "from sklearn.decomposition import NMF \n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset con el que vamos a trabajar desde la carpeta de la materia. Vamos a utilizar el dataset de la clase de sentiment sobre letras de distintos ritmos musicales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataframe que generamos previamente\n",
    "data_corpus = pd.read_pickle(r\"C:\\Users\\HP\\Documents\\Ciencia De Datos\\Análisis de los funerales de la mamá grande\\corpus.pkl\")\n",
    "data_corpus.head()\n",
    "text=[]\n",
    "for i in range(0, len(data_corpus.Cuentos.keys())):\n",
    "    text.append(data_corpus.Cuentos[i])\n",
    "\n",
    "# Pasar el dataFrame a una estructura de filas y columnas \n",
    "data_corpus={'title':data_corpus.Cuentos.keys(), \n",
    "             'text':text\n",
    "             }\n",
    "data_corpus=pd.DataFrame(data_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de la matriz documentos-términos\n",
    "\n",
    "Vamos a construir esta matriz con valorización tf-idf, es decir, además de la frecuencia del término vamos a ponderar la especificidad. \n",
    "Esto lo hacemos en dos pasos: primero, describimos nuestros datos mediante frecuencia de términos; luego, le agregamos la valorización de la especificidad. \n",
    "Vamos además a remover las *stopwords* obtenidas de *nltk*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2253)\n"
     ]
    }
   ],
   "source": [
    "# Lista de stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# Creamos el objeto contador de palabras, pidiéndole que remueve\n",
    "# las stopwords, los términos que aparecen en un único documento (min_df)\n",
    "# y los términos que aparecen en más del 70% de los documentos (max_df).\n",
    "# Esto es para eliminar palabras raras (o errores de tipeo) y \n",
    "# términos que seguramente son stopwords no incluídos en la lista\n",
    "count = CountVectorizer(min_df = 2, max_df = 0.70, stop_words = stopwords)\n",
    "\n",
    "# Ajustamos con los datos. Acá especificamente creamos una matriz documentos-términos\n",
    "x_count = count.fit_transform(data_corpus['text'])\n",
    "\n",
    "# Dimensions de la matriz doc-tér\n",
    "print(x_count.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos tranquilamente trabajar sobre la matriz de documentos descritos a través de la frecuencia de las palabras o bien, como es habitual, ponderar la especificidad de los términos mediante tf-idf. Esto lo hacemos de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto tf-idf. Le decimos además que devuelva los\n",
    "# vectores documento con norma euclídea igual a 1 (norm = 'l2')\n",
    "tfidf = TfidfTransformer(norm = 'l2')\n",
    "\n",
    "# Creamos la matriz tf-idf a partir de la matriz de frecuencias\n",
    "x_tfidf = tfidf.fit_transform(x_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación de NMF sobre el corpus\n",
    "\n",
    "Vamos a buscar los tópicos en nuestro corpus de textos a través de la descomposición en matrices no-negativas. Elijamos por ejemplo 5 tópicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "# Elijamos la cantidad de tópicos\n",
    "n_components = 5\n",
    "\n",
    "# Construímos el objeto NMF con los tópicos indicados \n",
    "nmf = NMF(n_components = n_components)\n",
    "\n",
    "# Aplicamos sobre nuestros datos\n",
    "x_nmf = nmf.fit_transform(x_tfidf)\n",
    "\n",
    "# Dimensión de la matriz transformada\n",
    "print(x_nmf.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué significa cada tópico. Lo que hay que indentificar es cuáles son los índices con mayor peso en cada componente y a qué término le corresponde. Para ello primero vamos a invertir el diccionario de vocabulario (para obtener otro del estilo \"índice: término\") y ordenar los índices de mayor a menor en cada componente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcadio, josé, úrsula, buendía, melquíades, aldea, gitanos, aureliano, laboratorio, hombres\n",
      "\n",
      "\n",
      "niña, tren, pueblo, padre, pájaros, madre, sacerdote, calor, estación, señora\n",
      "\n",
      "\n",
      "montiel, jaula, josé, viuda, médico, señor, úrsula, niño, ricos, esposa\n",
      "\n",
      "\n",
      "alcalde, café, gaveta, muela, viernes, abuela, junto, silla, sacó, ratones\n",
      "\n",
      "\n",
      "mamá, reina, macondo, antonio, isabel, autoridad, aquella, padre, siglo, familia\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Objeto índice: término de nuestro vocabulario\n",
    "vocabulary = {item: key for key, item in count.vocabulary_.items()}\n",
    "\n",
    "# Para cada componente\n",
    "for n in range(n_components):\n",
    "\n",
    "  # Ordenamos una lista del largo de nuestro vocabulario según el peso en cada componente y nos quedamos con los primeros 10\n",
    "  list_sorted = sorted(range(nmf.components_.shape[1]), reverse = True, key = lambda x: nmf.components_[n][x])[:10]\n",
    "\n",
    "  # Printeamos los términos asociados a los valores más grande de cada una de las componentes\n",
    "  print(', '.join([vocabulary[i] for i in list_sorted]))\n",
    "  print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que sea más fácil visualizar, armemos wordclouds, poniendole el peso dado por el algoritmo de NMF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordClouds\n",
    "wc_atributos = {'height' : 800,\n",
    "                'width' : 1200,\n",
    "                'background_color' : 'white',\n",
    "                'max_words' : 20\n",
    "                } # Defino los parámetros que les voy a pasar a los wordclouds\n",
    "\n",
    "# Creo la figura\n",
    "fig, axs = plt.subplots(n_components, figsize = (6,20))\n",
    "\n",
    "# Recorro para todas las componentes\n",
    "for n in range(n_components):\n",
    "\n",
    "  # 10 términos más pesados\n",
    "  list_sorted = sorted(range(len(vocabulary)), reverse = True, key = lambda x: nmf.components_[n][x])[:10]\n",
    "\n",
    "  # Diccionario término: peso\n",
    "  comp_dict = {vocabulary[i]: nmf.components_[n][i] for i in list_sorted}\n",
    "\n",
    "  # Creo el wordlcoud\n",
    "  wc = WordCloud(**wc_atributos # De esta forma, le estoy diciendo a la función que expanda el diccionario de atributos de forma tal de que entienda lo que quiero que haga\n",
    "                 ).generate_from_frequencies(comp_dict)\n",
    "\n",
    "  axs[n].set_title('Tópico {}'.format(n))\n",
    "  axs[n].imshow(wc)\n",
    "  axs[n].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolución de los tópicos\n",
    "\n",
    "Algo que podemos hacer al identificar los tópicos es ver cómo evolucionan en el tiempo, es decir, si hay tópicos dominantes en determinados períodos. Antes de hacer esto, normalizemos los vectores documentos en el espacio de tópicos para interpretarlos como una distribución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizador\n",
    "from sklearn.preprocessing import Normalizer \n",
    "\n",
    "# Creamos un objeto para normalizar a que la suma dé 1\n",
    "norm = Normalizer('l1')\n",
    "\n",
    "# Sobreescribimos sobre la matriz de documentos-tópicos\n",
    "x_nmf = norm.fit_transform(x_nmf)\n",
    "\n",
    "# Guardemos en el dataframe esta información\n",
    "for n in range(n_components):\n",
    "  df['nmf_comp{}'.format(n)] = x_nmf[:,n]\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
